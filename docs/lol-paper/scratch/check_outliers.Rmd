---
title: "check outliers"
author: "Eric Bridgeford"
date: "June 5, 2018"
output: html_document
---

```{r, message=FALSE}
require(lolR)
require(ggplot2)
require(MASS)
require(abind)
require(robust)
n=100
d=40
r=7
#rot=-pi/4  # rotation in radians
#R = rbind(c(cos(rot), -sin(rot), 0, 0, 0), c(sin(rot), cos(rot), 0, 0, 0),
#          c(0,0,1,0,0), c(0,0,0,1,0), c(0,0,0,0,1))  # rotation matrix for rotation by 45 degrees in first 2 dimensions
```

# Generate Data

```{r}
data <- lol.sims.rev_rtrunk(n, d, b=1, maxvar=6)
data.regular_trunk <- lol.sims.rtrunk(n, d)
X = data$X; Y = data$Y

# generate outliers
Sigmas.outliers <- data.regular_trunk$Sigmas
for (i in 1:dim(data.regular_trunk$Sigmas)[3]) {
  for (j in 1:dim(data.regular_trunk$Sigmas)[1]) {
    for (k in 1:dim(data.regular_trunk$Sigmas)[2]) {
      Sigmas.outliers[j, k, i] <- data.regular_trunk$Sigmas[(d+1)-j, (d+1)-k, i]
    }
  }
}
```

## Average Per-Class Error as a function of Magnitude of the Noise

```{r}
result.robcov.performance <- data.frame(i=c(), class=c(), error=c(), method=c(), iter=c())
niter <- 10
mus.outliers <- array(0, dim=c(d, 2))
for (i in 1:10) {
  for (j in 1:niter) {
    data.outlier <- lolR:::lol.sims.sim_gmm(mus=mus.outliers,  Sigmas=i*Sigmas.outliers, n=.2*n, priors=data$priors)
    # randomly reorder Y for "noise"
    X.o <- data.outlier$X; Y.o <- sample(data.outlier$Y)
    
    X <- rbind(X, X.o)
    Y <- c(Y, Y.o)
    # randomly reorder X and Y
    reord <- sample(1:length(Y))
    X <- X[reord,]; Y <- Y[reord]
    for (c in 1:2) {
      result.robcov.performance <- rbind(result.robcov.performance, data.frame(i=i, class=c,
                                                                               error=norm(cov(X[Y == c,,drop=FALSE]) - data$Sigmas[,,c], "F"),
                                                                               method="cov", iter=j))
      result.robcov.performance <- rbind(result.robcov.performance, data.frame(i=i, class=c,
                                                                               error=norm(covRob(X[Y == c,,drop=FALSE], estim="weighted")$cov - data$Sigmas[,,c], "F"),
                                                                               method="robCov", iter=j))
    }
    
  }
}

robcov.perf <- aggregate(error ~ i + method, data=result.robcov.performance, FUN=mean)
ggplot(data=robcov.perf, aes(x=i, y=error, group=method, color=method)) +
  geom_line() +
  xlab("Variance Multiplier of Noise Points") +
  ylab("Error of True Covariance Estimation") +
  ggtitle("Comparison of Robust and non-robust Covariance Estimation")
```

```{r}
data <- lol.sims.rev_rtrunk(n, d, b=1, maxvar=6)
data.regular_trunk <- lol.sims.rtrunk(n, d)
X = data$X; Y = data$Y

# generate outliers
Sigmas.outliers <- data.regular_trunk$Sigmas
for (i in 1:dim(data.regular_trunk$Sigmas)[3]) {
  for (j in 1:dim(data.regular_trunk$Sigmas)[1]) {
    for (k in 1:dim(data.regular_trunk$Sigmas)[2]) {
      Sigmas.outliers[j, k, i] <- data.regular_trunk$Sigmas[(d+1)-j, (d+1)-k, i]
    }
  }
}
mus.outliers <- array(0, dim=c(d, 2))
data.outlier <- lolR:::lol.sims.sim_gmm(mus=mus.outliers,  Sigmas=10*Sigmas.outliers, n=.2*n, priors=data$priors)
# randomly reorder Y for "noise"
X.o <- data.outlier$X; Y.o <- sample(data.outlier$Y)

X <- rbind(X, X.o)
Y <- c(Y, Y.o)
# randomly reorder X and Y
reord <- sample(1:length(Y))
X <- X[reord,]; Y <- Y[reord]
X.train <- X[1:(length(Y)/2),]; X.test <- X[(length(Y)/2 + 1):length(Y),]
Y.train <- Y[1:(length(Y)/2)]; Y.test <- Y[(length(Y)/2 + 1):length(Y)]
```

```{r}
data <- data.frame(x1=X[,1], x2=X[,2], y=Y)
data$y <- factor(data$y)
ggplot(data, aes(x=x1, y=x2, color=y)) +
  geom_point() +
  xlab("x1") +
  ylab("x2") +
  ggtitle("Simulated Data")
```

# Regular LOL

```{r}
result <- lol.project.lol(X.train, Y.train, r)

data <- data.frame(x1=result$Xr[,1], x2=result$Xr[,2], y=Y.train)
data$y <- factor(data$y)
ggplot(data, aes(x=x1, y=x2, color=y)) +
  geom_point(alpha=0.6) +
  xlab("x1") +
  ylab("x2") +
  ggtitle("Projected Training Data using LOL")
```

```{r}
newXr <- lol.embed(X.test, result$A)

data <- data.frame(x1=newXr[,1], x2=newXr[,2], y=Y.test)
data$y <- factor(data$y)
ggplot(data, aes(x=x1, y=x2, color=y)) +
  geom_point(alpha=0.6) +
  xlab("x1") +
  ylab("x2") +
  ggtitle("Projected Testing Data using LOL")
```

```{r, fig.width=5}
liney <- MASS::lda(result$Xr, Y.train)
result <- predict(liney, newXr)
lhat <- 1 - sum(result$class == Y.test)/length(Y.test)

data <- data.frame(x1=result$x[,1], y=Y.test)
data$y <- factor(data$y)
ggplot(data, aes(x=x1, fill=y)) +
  geom_density(adjust=1.5, alpha=0.6) +
  xlab("$x_1$") +
  ylab("Density") +
  ggtitle(sprintf("LOL-LDA, L = %.2f", lhat))
```

# Robust LOL

```{r}
result <- lol.project.lol(X.train, Y.train, r, robust=TRUE)

data <- data.frame(x1=result$Xr[,1], x2=result$Xr[,2], y=Y.train)
data$y <- factor(data$y)
ggplot(data, aes(x=x1, y=x2, color=y)) +
  geom_point(alpha=0.6) +
  xlab("x1") +
  ylab("x2") +
  ggtitle("Projected Data using Robust LOL")
```

```{r}
newXr <- lol.embed(X.test, result$A)

data <- data.frame(x1=newXr[,1], x2=newXr[,2], y=Y.test)
data$y <- factor(data$y)
ggplot(data, aes(x=x1, y=x2, color=y)) +
  geom_point(alpha=0.6) +
  xlab("x1") +
  ylab("x2") +
  ggtitle("Projected Testing Data using Robust LOL")
```

```{r, fig.width=5}
liney <- MASS::lda(result$Xr, Y.train)
result <- predict(liney, newXr)
lhat <- 1 - sum(result$class == Y.test)/length(Y.test)

data <- data.frame(x1=result$x[,1], y=Y.test)
data$y <- factor(data$y)
ggplot(data, aes(x=x1, fill=y)) +
  geom_density(adjust=1.5, alpha=0.6) +
  xlab("$x_1$") +
  ylab("Density") +
  ggtitle(sprintf("RLOL-LDA, L = %.2f", lhat))
```

# PLS

```{r}
result <- lol.project.pls(X.train, Y.train, r)

data <- data.frame(x1=result$Xr[,1], x2=result$Xr[,2], y=Y.train)
data$y <- factor(data$y)
ggplot(data, aes(x=x1, y=x2, color=y)) +
  geom_point(alpha=0.6) +
  xlab("x1") +
  ylab("x2") +
  ggtitle("Projected Data using PLS")
```

```{r}
newXr <- lol.embed(X.test, result$A)

data <- data.frame(x1=newXr[,1], x2=newXr[,2], y=Y.test)
data$y <- factor(data$y)
ggplot(data, aes(x=x1, y=x2, color=y)) +
  geom_point(alpha=0.6) +
  xlab("x1") +
  ylab("x2") +
  ggtitle("Projected Testing Data using PLS")
```

```{r, fig.width=5}
liney <- MASS::lda(result$Xr, Y.train)
result <- predict(liney, newXr)
lhat <- 1 - sum(result$class == Y.test)/length(Y.test)

data <- data.frame(x1=result$x[,1], y=Y.test)
data$y <- factor(data$y)
ggplot(data, aes(x=x1, fill=y)) +
  geom_density(adjust=1.5, alpha=0.6) +
  xlab("$x_1$") +
  ylab("Density") +
  ggtitle(sprintf("PLS-LDA, L = %.2f", lhat))
```

